{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2287e1",
   "metadata": {},
   "source": [
    "#### I have implemented self-attention, multi-headed self attention and finally the transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "370a17db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/payal/Desktop/Generalized-ML/my_env/lib/python3.13/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab51a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim):\n",
    "        super(self_attention,self).__init__()\n",
    "            \n",
    "        self.embed_dim = embed_dim\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        Q = self.query(x) ## Batch x Seq_len x Embed_dim\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        ## Q = Batch x Seq_len x Embed_dim\n",
    "        ## K_t= Batch x Embed_dim x Seq_len\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2,-1)) / (self.embed_dim ** 0.5)\n",
    "        attn_weight = F.softmax(attn_scores, dim=-1) ## Batch x Seq_len x Seq_len\n",
    "\n",
    "        out = torch.matmul(attn_weight, V) ## Batch x Seq_len x Embed_dim\n",
    "        return out, attn_weight\n",
    "    \n",
    "\n",
    "#  Q, K, V are linear projections of the input.\n",
    "\n",
    "# We take dot product QK^T to measure similarity between tokens.\n",
    "\n",
    "# Divide by √(d_k) for numerical stability.\n",
    "\n",
    "# Softmax → weights → weighted sum of values (context vector).\n",
    "\n",
    "# Output has same shape as input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02ade477",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiheadedattention(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_dim,num_heads):\n",
    "        super(multiheadedattention ,self).__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)  ## Final output linear layer to combine heads\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        B, T, D = x.size()\n",
    "\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "\n",
    "        # Resshape into (B, num_heads, T, head_dim)\n",
    "        Q = Q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_output = torch.matmul(attention_weights, V) ## (B, num_heads, T, head_dim)\n",
    "\n",
    "        ## concatenate heads\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(B, T, D)\n",
    "        out = self.out(attention_output)  ## Final linear layer\n",
    "\n",
    "        return out, attention_weights\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d9c227a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feedforward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_hidden_dim, dropout=0.1):\n",
    "        super(feedforward,self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(embed_dim, ff_hidden_dim),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(ff_hidden_dim, embed_dim),\n",
    "                                 nn.Dropout(dropout))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12f32670",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformerblock(nn.Module):\n",
    "    def __init__(self, embed_dim,num_heads,ff_hidden_dim, dropout=0.1):\n",
    "        super(transformerblock,self).__init__()\n",
    "        self.attention = multiheadedattention(embed_dim,num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.feed_forward = feedforward(embed_dim, ff_hidden_dim, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        attention_out, _ = self.attention(x)\n",
    "        x = x+ self.dropout(attention_out) ## Residual connection\n",
    "        x = self.norm1(x)\n",
    "\n",
    "\n",
    "        ff_out = self.feed_forward(x)\n",
    "        x = x + self.dropout(ff_out)  ## Residual connection\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d3b0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
